- 中高级大数据数仓简历编写
  - 自我介绍
  - 职业技能
    - linux,java,shell,python
    - 大数据基础框架，hadoop,hive,spark,flink,kafka,clickhouse
    - 数仓搭建理论：离线，实时，数据治理
    - 修饰语：精通，了解，掌握，熟悉
    - 高级：设计方面能力，从0-1处理过多少亿级别数据，具备设计，调优能力，数据治理。源码读写能力，二次开发能力
  - 工作经验
  - 项目介绍——不在于多，在于精
    - 项目名称：注意项目名称高大上
    - 项目描述：完整表现出项目干啥的，解决什么问题
    - 技术：精通和掌握级别的技术
    - 项目职责：
      - 数据源
      - 数仓搭建：离线，实时：每个层次的处理——（不要太笼统，具体些）
      - BI报表
      - 数据治理：【高级】
      - 对数据源-改造能力，二次开发（数据开发）
  - 自我评价
    - 抗压能力，团结合作，工作认真

- 面试
  - 背景：数仓，ETL，实时，开发工程师
  - 理解，必须掌握，能口述一遍
  - 数仓工程师：1.1-1.9，2-6章
  - 实时，开发工程师

- Linux
  - 常用命令：find,ps ,top ,netstat,tail , less ,ping ,telnet
  - 命令
    - 查看内存 top
    - 查看磁盘存储情况： df -h
    - 查看你磁盘io读写情况，iotop（需要安装一下），iotop -o
    - 查看端口占用情况 netstat -tunlp | grep 端口号
    - 查看进程 ps aux
  - 使用Linux命令查询file1中空行所在的行号： `awk '/^$/{print NR}' file1.txt `
  - 使用Linux命令计算第二列的和并输出 `cat chengji.txt | awk -F " " '{sum+=$2} END{print sum}'`
  - Shell脚本里如何检查一个文件是否存在？如果不存在该如何处理？
    - ```
      #!/bin/bash
      if [ -f file.txt ]; then echo "文件存在!"
      else
      echo "文件不存在!"
      fi
      ```
  - 用shell写一个脚本，对文本中无序的一列数字排序并打印求和 `sort -n test.txt|awk '{a+=$0;print $0}END{print "SUM="a}'`
  - 请用shell脚本写出查找当前文件夹（/home）下所有的文本文件内容中包含有字符”shen”的文件名称 `grep -r "shen" /home | cut -d ":" -f 1`

- hadoop基础
  - 大数据基础环境
    - 简要描述如何安装配置apache的一个开源Hadoop，只描述即可，无需列出具体步骤，列出具体步骤更好
        1. 准备三台客户机（配置IP，配置主机名...）
        2. 安装jdk，安装hadoop
        3. 配置JAVA_HOME和HADOOP_HOME
        4. 使每个节点上的环境变量生效（source /etc/profile）
        5. 准备分发脚本 xsync
           a. **在/user/atguigu/bin下创建脚本：xsync
        6. 明确集群的配置
        7. 修改配置文件
           a. **core-site.xml
           b. **hadoop-env.sh
           c. **hdfs-site.xml
           d. **yarn-env.sh
           e. **yarn-site.xml
           f. **mapred-env.sh
           g. **mapred-site.xml
           h. **配置slaves
        8. 分发配置文件
           a. **xsync /etc/hadoop
        9. 删掉data和logs文件夹
        10. 配置ssh（hadoop102，hadoop103）
        11. 分发配置文件
        12. 格式化hdfs（hdfs namenode -format）
        13. 群启hdfs
    - Hadoop中需要哪些配置文件，其作用是什么？
      - core-site.xml: 
      - ZooKeeper集群的地址和端口。注意，数量一定是奇数，且不少于三个节点
    - 请列出正常工作的Hadoop集群中Hadoop都分别需要启动哪些进程，它们的作用分别是什么
      1. NameNode它是hadoop中的主服务器，管理文件系统名称空间和对集群中存储的文件的访问，保存有metadate。
      2. SecondaryNameNode它不是namenode的冗余守护进程，而是提供周期检查点和清理任务。帮助NN合并editslog，减少NN启动时间。
      3. DataNode它负责管理连接到节点的存储（一个集群中可以有多个节点）。每个存储数据的节点运行一个datanode守护进程。
      4. ResourceManager（JobTracker）JobTracker负责调度DataNode上的工作。每个DataNode有一个TaskTracker，它们执行实际工作
      5. NodeManager（TaskTracker）执行任务。
      6. DFSZKFailoverController高可用时它负责监控NN的状态，并及时的把状态信息写入ZK。   它通过一个独立线程周期性的调用NN上的一个特定接口来获取NN的健康状态。FC也有选择谁作为Active NN的权利，因为最多只有两个节点，目前选择策略还比较简单（先到先得，轮换）。
      7. JournalNode 高可用情况下存放namenode的editlog文件。
    - 简述Hadoop的几个默认端口及其含义。
      - 1）dfs.namenode.http-address:50070
        2）SecondaryNameNode辅助名称节点端口号：50090
        3）dfs.datanode.address:50010
        1.fs.defaultFS:8020 或者9000
        2.yarn.resourcemanager.webapp.address:8088
    - HDFS的存储机制（读写流程）
      - 写入
      - ![](../jpg/img_25.png)
      - 1）客户端向namenode请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。
        2）namenode返回是否可以上传。
        3）客户端请求第一个 block上传到哪几个datanode服务器上。
        4）namenode返回3个datanode节点，分别为dn1、dn2、dn3。
        5）客户端请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管  道建立完成。
        6）dn1、dn2、dn3逐级应答客户端
        7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答
        8）当一个block传输完成之后，客户端再次请求namenode上传第二个block的服务器。（重复  执行3-7步）
      - 读取
      - ![](../jpg/img_26.png)
      - 1.客户端向namenode请求下载文件，namenode通过查询元数据，找到文件块所在的datanode
        地址。
        2.挑选一台datanode（就近原则，然后随机）服务器，请求读取数据。
        3.datanode开始传输数据给客户端（从磁盘里面读取数据放入流，以packet为单位来做校验。
        4.客户端以packet为单位接收，先在本地缓存，然后写入目标文件。
    - SecondaryNameNode 工作机制
      1. 第一阶段：namenode启动
         1. 第一次启动namenode格式化后，创建fsimage和edits文件。如果不是第一次启动，  直接加载编辑日志和镜像文件到内存。
         2. 客户端对元数据进行增删改的请求
            1. namenode记录操作日志，更新滚动日志。
            2. namenode在内存中对数据进行增删改查。
      2. 第二阶段：Secondary NameNode工作
         1. 拷贝fsimage.chkpoint到namenode
         2. Secondary NameNode询问namenode是否需要checkpoint。直接带回namenode是否检查结果。
         3. Secondary NameNode请求执行checkpoint。
         4. namenode滚动正在写的edits日志
         5. 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode
         6. Secondary NameNode加载编辑日志和镜像文件到内存，并合并。
         7. 生成新的镜像文件fsimage.chkpoint
         8. namenode将fsimage.chkpoint重新命名成fsimage
    - NameNode与SecondaryNameNode 的区别与联系
        1. 机制流程同上；
        2. 区别
           a. NameNode负责管理整个文件系统的元数据，以及每一个路径（文件）所对应的数  据块信息。
           b. SecondaryNameNode主要用于定期合并命名空间镜像和命名空间镜像的编辑日志。
        3. 联系：
           1. SecondaryNameNode中保存了一份和namenode一致的镜像文件（fsimage）和编  辑日志（edits）。
           2. 在主namenode发生故障时（假设没有及时备份数据），可以从SecondaryNameNode
              恢复数据。
    - 服役新数据节点和退役旧节点步骤
      - 节点上线
      - 节点下线
    - Namenode挂了怎么办
      - 将SecondaryNameNode中数据拷贝到namenode存储数据的目录
      - 使用-importCheckpoint选项启动namenode守护进程，从而将SecondaryNameNode    中数据拷贝到namenode目录中。
    - Namenode HA高可用
      - 高可用，是保证业务连续性的有效解决方案,一般有两个或两个以上的节点，分为活动节点（Active）及备用节点（Standby））。用于实现业务的不中断或短暂中断
      - QJM/Qurom Journal Manager,基本原理就是用 2N+1 台 JournalNode 存储 EditLog，每次写数据操作有>=N+1 返回成功时即认为该次写成功，数据不会丢失了
      - FailoverController 主要包括三个组件:
        - HealthMonitor: 监控 NameNode 是否处于 unavailable 或 unhealthy 状态
        - ActiveStandbyElector: 监控 NN 在 ZK 中的状态
        - ZKFailoverController: 订阅 HealthMonitor 和 ActiveStandbyElector 的事件，并管理 NN 的状态
          - 健康监测：周期性的向它监控的 NN 发送健康探测命令
          - 会话管理：如果 NN 是健康的，zkfc 就会在 zookeeper 中保持一个打开的会话，如果 NameNode 同时还是 Active 状态的，那么 zkfc 还会在 Zookeeper 中占有一个类型为短暂类型的 znode，当这个 NN 挂掉时，这个 znode 将会被删除，然后备用的NN 将会得到这把锁，升级为主 NN，同时标记状态为 Active
        - master 选举：通过在 zookeeper 中维持一个短暂类型的 znode，来实现抢占式的锁机制，从而判断那个 NameNode 为 Active 状态
    - yarn HA
      - Hadoop 2.4.0版本开始，Yarn 实现了 ResourceManager HA
      - 由于资源使用情况和 NodeManager 信息都可以通过 NodeManager 的心跳机制重新构建出来，因此只需要对 ApplicationMaster 相关的信息进行持久化存储即可
      - 在一个典型的 HA 集群中，两台独立的机器被配置成 ResourceManger。在任意时间，有且只允许一个活动的 ResourceManger,另外一个备用。切换分为两种方式
    - HA Hadoop搭建
    - 单点故障与”脑裂”
      - ![](../jpg/img_27.png)
      - client的事务性操作对HA提供了支持
      - QJM
    - Hadoop 节点分布
      - ![](../jpg/img_28.png)
    - 联邦的实现
      - 为什么：Hadoop的NN所使用的资源受所在服务的物理限制，不能满足实际生产需求
      - 实现：采用多台NN组成联邦。NN是独立的，NN之间不需要相互调用。NN是联合的，同属于一个联邦，所管理的DN作为block的公共存储
      - 主要优点：
        - ● 命名空间可伸缩性——联合添加命名空间水平扩展。DN也随着NN的加入而得到拓展。
          ● 性能——文件系统吞吐量不是受单个Namenode限制。添加更多的Namenode集群扩展文件    系统读/写吞吐量。
          ● 隔离——隔离不同类型的程序，一定程度上控制资源的分配
      - 配置
      - 操作
  - mapreduce
    - 谈谈Hadoop序列化和反序列化及自定义bean对象实现序列化
      - 序列化和反序列化
        - 序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久 化）和网络传输。
        - 反序列化就是将收到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成 内存中的对象
        - Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以， hadoop自己开发了一套序列化机制（Writable），精简、高效
      - 自定义bean对象要想序列化传输步骤及注意事项
          a. 必须实现Writable接口
          b. 反序列化时，需要反射调用空参构造函数，所以必须有空参构造
          c. 重写序列化方法
          d. 重写反序列化方法
          e. 注意反序列化的顺序和序列化的顺序完全一致
        - 要想把结果显示在文件中，需要重写toString()，且用”\t”分开，方便后续用
      - 如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序
    - FileInputFormat切片机制
      1. 简单地按照文件的内容长度进行切片
      2. 切片大小，默认等于block大小
      3. 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片
    - 自定义InputFormat流程
      1. 自定义一个类继承FileInputFormat
      2. 改写RecordReader，实现一次读取一个完整文件封装为KV
    - 如何决定一个job的map和reduce的数量
      - map
        - splitSize=max{minSize,min{maxSize,blockSize}}
        - map数量由处理的数据分成的block数量决定default_num = total_size / split_size;
      - reduce
        - reduce的数量job.setNumReduceTasks(x);x 为reduce的数量。不设置的话默认为 1
    - Maptask的个数由什么决定
      - 一个job的map阶段MapTask并行度（个数），由客户端提交job时的切片个数决定
    - MapTask工作机制
      1. Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。
      2. Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一    系列新的key/value。
      3. Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。
      4. Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必     要时对数据进行合并、压缩等操作。
      - 溢写阶段
        - 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。
        - 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每    个分区中的数据进行一次聚集操作。
        - 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销
    - ReduceTask工作机制
      1. Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果   其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。
      2. Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上   的文件进行合并，以防止内存使用过多或磁盘上文件过多。
      3. Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一   组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。
      4. Reduce阶段：reduce()函数将计算结果写到HDFS上。
    - 请描述mapReduce有几种排序及排序发生的阶段
      - 部分排序： MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序
      - 全排序：
        - 最简单的方法是使用一个分区。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce所提供的并行架构
        - 首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路是使用一个分区来描述输出的全局排序
      - 辅助排序：有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序
      - 二次排序：如果compareTo中的判断条件为两个即为二次排序
      - 自定义排序
    - 请描述mapReduce中shuffle阶段的工作流程，如何优化shuffle阶段
      - 分区，排序，溢写，拷贝到对应reduce机器上，增加combiner，压缩溢写的文件
      - ![](../jpg/img_29.png)
    - 请描述mapReduce中combiner的作用是什么，一般使用情景，哪些情况不需要，及和reduce的区别
      1. Combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量。
      2. Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟reducer的输入kv类型要对应起来。
      3. Combiner和reducer的区别在于运行的位置。Combiner是在每一个maptask所在的节点运行； Reducer是接收全局所有Mapper的输出结果。
    - Mapreduce的工作原理，请举例子说明mapreduce是怎么运行的
      - 如果没有定义partitioner，那数据在被送达reducer前是如何被分区的
      - 如果没有自定义的 partitioning，则默认的 partition 算法，即根据每一条数据的 key 的 hashcode 值摸运算（%）reduce 的数量，得到的数字就是“分区号”
    - MapReduce 怎么实现 TopN
      - 可以自定义groupingcomparator，或者在map端对数据进行排序，然后再reduce输出时，控制只输出前n个数。就达到了topn输出的目的
    - 有可能使 Hadoop 任务输出到多个目录中么？如果可以，怎么做
      - 可以输出到多个目录中，采用自定义OutputFormat
      - a. 自定义outputformat，
          b. 改写recordwriter，具体改写输出数据的方法write()
    - 简述hadoop实现join的几种方法及每种方法的实现
      - reduce side join
        - Map端的主要工作：为来自不同表(文件)的key/value对打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。
        - 两个数据集分别由两个不同的 MapReduce 作业处理，然后通过 Reduce 阶段进行连接。其中一个数据集被标记为“小”数据集（可能不是完全小于内存），而另一个是“大”数据集。在 Map 阶段，每个数据集都会被映射为键值对，其中键用于将数据发送到正确的 Reduce 任务中，以便进行连接
        - Reduce端的主要工作：在reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在map阶段已经打标志)分开，最后进行合并就ok了
      - map side join
        -  当一个数据集比较小，可以完全装载到内存中时，可以使用 Map-Side Join。在这种情况下，小的数据集被加载到内存中，然后在 Map 阶段与大的数据集进行连接。这种方法避免了 Reduce 阶段，从而提高了性能
        - 具体办法：采用distributedcache
          a. 在mapper的setup阶段，将文件读取到缓存集合中。
          b. 在驱动函数中加载缓存。
        - job.addCacheFile(new URI("file:/e:/mapjoincache/pd.txt"));// 缓存普通文件到task运行
      - Distributed Cache Join
        - 使用分布式缓存（Distributed Cache）来将小数据集分发到所有的 Map 任务中
    - 请简述hadoop怎样实现二级排序
      - 对map端输出的key进行排序，实现的compareTo方法。 在compareTo方法中排序的条件有二个
    - 参考下面的MR系统的场景
      - 有三个文件的大小分别是:64KB 130MB 260MB Hadoop框架会把这些文件拆分为多少块
        - 4块：64K，130M，128M，132M
      - Hadoop中RecordReader的作用是什么
        - 负责将输入数据分割、解析并转换成键值对，以供 Map 任务进行处理
  - yarn
    - 述Hadoop1与Hadoop2 的架构异同
      - 加入了yarn解决了资源调度的问题。
      - 加入了对zookeeper的支持实现比较可靠的高可用
    - 为什么会产生yarn,它解决了什么问题，有什么优势
      - Yarn最主要的功能就是解决运行的用户程序与yarn框架完全解耦。
      - Yarn上可以运行各种类型的分布式运算程序（mapreduce只是其中的一种），比如mapreduce、storm程序，spark程序……
    - MR作业提交全过程
      - ![](../jpg/img_30.png)
    - HDFS的数据压缩算法？及每种算法的应用场景
      - gzip压缩:
        - 压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格    式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便
        - 不支持split
      - Bzip2压缩
        - 支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native； 在linux系统下自带bzip2命令，使用方便
        - 压缩/解压速度慢；不支持native
      - Lzo压缩
        - 压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；    可以在linux系统下安装lzop命令，使用方便
        - 压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需    要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）
      - Snappy压缩
        - 高速压缩速度和合理的压缩率
        - 不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装
    - Hadoop的调度器总结
      - 先进先出调度器（FIFO）
      - 容量调度器（Capacity Scheduler）:
        - 支持多个队列，每个队列用FIFO，
      - 公平调度器（Fair Scheduler）
        - 按照优先级分配
        - 缺额越大，优先获得资源
    - mapreduce推测执行算法及原理
      - 作业完成时间取决于最慢的任务完成时间
      - 推测执行机制
        - 发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果
        1. 执行推测任务的前提条件
        2. 每个task只能有一个备份任务；
        3. 当前job已完成的task必须不小于0.05（5%）
        4. 开启推测执行参数设置。Hadoop2.7.2 mapred-site.xml文件中默认是打开的。
        - 不能启用推测执行机制情况
          - a. 任务间存在严重的负载倾斜；
              b. 特殊任务，比如任务向数据库中写数据
  - Hadoop优化
    - mapreduce 跑的慢的原因？
      - 计算机性能：CPU、内存、磁盘健康、网络
      - I/O 操作优化
        1. 数据倾斜
        2. map和reduce数设置不合理
        3. reduce等待过久
        4. 小文件过多
        5. 大量的不可分块的超大文件
        6. spill次数过多
        7. merge次数过多等。
    - mapreduce 优化方法
      1. 数据输入：
           a. 合并小文件：在执行mr任务前将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载次数，而任务的装载比较耗时，从而导致 mr 运行较慢。
           b. 采用ConbinFileInputFormat来作为输入，解决输入端大量小文件场景。
      2. map阶段
         1. 减少spill次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发spill的内存上限，    减少spill次数，从而减少磁盘 IO。
         2. 减少merge次数：通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次    数，从而缩短mr处理时间。
         3. 在 map 之后先进行combine处理，减少 I/O。
      3. reduce阶段
      4. 合理设置map和reduce数：两个都不能设置太少，也不能设置太多。太少，会导致task等待，延长处理时间；太多，会导致 map、reduce任务间竞争资源，造成处理超时等错误
      5. 设置map、reduce共存：调整slowstart.completedmaps参数，使map运行到一定程度后， reduce也开始运行，减少reduce的等待时间。
      6. 规避使用reduce，因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。
      7. 合理设置reduc端的buffer，默认情况下，数据达到一个阈值的时候，buffer中的数据就    会写入磁盘，然后reduce会从磁盘中获得所有的数据。也就是说，buffer和reduce是没有直接关联的，中间多个一个写磁盘->读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得buffer 中 的 一 部 分 数 据 可 以 直 接 输 送 到 reduce， 从 而 减 少 IO 开 销 ： mapred.job.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读buffer中的数据直接拿给reduce使用。这样一来，设置buffer需要内存，读取数据需要内存， reduce计算也要内存，所以要根据作业的运行情况进行调整。
      8. IO传输
         （1）采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZOP压缩编码器。
         （2）使用SequenceFile二进制文件
      9. 数据倾斜问题
         1. 数据频率倾斜——某一个区域的数据量要远远大于其他区域。 
         2. 数据大小倾斜——部分记录的大小远远大于平均值。
         3. 如何收集倾斜数据
            1. 在reduce方法中加入记录map输出键的详细情况的功能
         4. 减少数据倾斜的方法
            1. 抽样和范围分区：可以通过对原始数据进行抽样得到的结果集来预设分区边界值
            2. 自定义分区：于输出键的背景知识进行自定义分区
            3. Combine：使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。在可能的情况下，combine的目的就是聚合并精简数据
    - HDFS小文件优化方法
      - 小文件弊端：HDFS上每个文件都要在namenode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用namenode的内存空间，另一方面就是索引文件过大是的索引速度变慢
      - 方案
        - Hadoop Archive：是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR    文件，这样在减少namenode内存使用
        - Sequence file：由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件
        - CombineFileInputFormat：一种新的inputformat，用于将多个文件合并成一个单独的split，    另外，它会考虑数据的存储位置
        - 开启JVM重用：对于大量小文件Job，可以开启JVM重用会减少45%运行时间。  
          - JVM重用理解：一个map运行一个jvm，重用的话，在一个map在jvm上运行完毕后，jvm继续运行其他jvm 
          - 具体设置：mapreduce.job.jvm.numtasks值在10-20之间
    - MapReduce怎么解决数据均衡问题，如何确定分区号
      - 解决方案同解决数据倾斜的方案
    - Hadoop中job和Tasks之间的区别是什么
      - 一个Mapreduce程序就是一个Job，而一个Job    里面可以有一个或多个Task，Task又可以区分为Map Task和Reduce Task.
- zookeeper
  - 请简述ZooKeeper的选举机制
    ⅰ. 服务器1启动，此时只有它一台服务器启动了，它发出去的报没有任何响应，所以它的选    举状态一直是LOOKING状态。
    ⅱ. 服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3)，所以服务器1、2还是继续保持LOOKING状态。
    ⅲ. 服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的Leader，而与上面    不同的是，此时有三台服务器选举了它，所以它成为了这次选举的Leader。
    ⅳ. 服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但    是由于前面已经有半数以上的服务器选举了服务器3，所以它成为Follower。
    ⅴ. 服务器5启动，同4一样成为Follower。
  - 客户端对ZooKeeper的ServerList的轮询机制
    - 随机，客户端在初始化( new ZooKeeper(String connectString, int sessionTimeout, Watcher watcher) )的过程中，将所有Server保存在一个List中，然后随机打散，形成一个环。之后从0号位开始一个一个使用
  - 客户端如何正确处理CONNECTIONLOSS(连接断开) 和SESSIONEXPIRED(Session 过期)两类连接异常
    - 发生CONNECTIONLOSS后，此时用户不需要关心我的会话是否可用，应用所要做的就是等待    客户端帮我们自动连接上新的zk机器，一旦成功连接上新的zk机器后，确认之前的操作是否执行成功了
  - 一个客户端修改了某个节点的数据，其他客户端能够马上获取到这个最新数据吗
    - ZooKeeper不能确保任何客户端能够获取（即Read Request）到一样的数据，除非客户端自己要求，方法是客户端在获取数据之前调用org.apache.zookeeper.AsyncCallbac k.VoidCallback, java.lang.Object) sync
    - ZK客户端A对 /my_test 的内容从 v1->v2, 但是ZK客户端B对 /my_test 的内容获取，依然得到的是 v1. 请注意，这个是实际存在的现象，当然延时很短。解决的方法是客户端B先调用 sync(), 再调用 getData()
  - ZooKeeper对节点的watch监听是永久的吗？为什么
    - 不是
    - 官方声明：一个Watch事件是一个一次性的触发器，当被设置了Watch的数据发生了改变的时    候，则服务器将这个改变发送给设置了Watch的客户端，以便通知它们
    - 一般是客户端执行getData(“/节点A”,true)，如果节点A发生了变更或删除，客户端会得到它的watch事件，但是在之后节点A又发生了变更，而客户端又没有设置watch事件，就不再给客户端发送
  - ZooKeeper中使用watch的注意事项有哪些
    ① Watches通知是一次性的，必须重复注册.
    ② 发生CONNECTIONLOSS之后，只要在session_timeout之内再次连接上（即不发生SESSIONEXPIRED），那么这个连接注册的watches依然在。
    ③ 节点数据的版本变化会触发NodeDataChanged，注意，这里特意说明了是版本变化。存在这样的情况，只要成功执行了setData()方法，无论内容是否和之前一致，都会触发NodeDataChanged。
    ④ 对某个节点注册了watch，但是节点被删除了，那么注册在这个节点上的watches都会被移除。
    ⑤ 同一个zk客户端对某一个节点注册相同的watch，只会收到一次通知。
    ⑥ Watcher对象只会保存在客户端，不会传递到服务端
  - 能否收到每次节点变化的通知
    - 如果节点数据的更新频率很高的话，不能
    - 当一次数据修改，通知客户端，客户端再次注册watch，在这个过程中，可能数据已经发生了许多次数据修改
  - 能否为临时节点创建子节点
    - ZooKeeper中不能为临时节点创建子节点，如果需要创建子节点，应该将要创建子节点的节点    创建为永久性节点
  - 是否可以拒绝单个IP对ZooKeeper的访问？如何实现
    - ZK本身不提供这样的功能，它仅仅提供了对单个IP的连接数的限制。你可以通过修改iptables    来实现对单个ip的限制
  - 在getChildren(String path, boolean watch)是注册了对节点子节点的变化，那么子节点的子节点变化能通知吗
    - 不能
  - 创建的临时节点什么时候会被删除，是连接一断就删除吗？ 延时是多少
    - 连接断了之后，ZK不会马上移除临时数据，只有当SESSIONEXPIRED之后，才会把这个会话建立的临时数据移除。因此，用户需要谨慎设置Session_TimeOut
  - ZooKeeper是否支持动态进行机器扩容？如果目前不支持，那么要如何扩容呢
    - ZooKeeper中的动态扩容其实就是水平扩容，Zookeeper对这方面的支持不太好，目前有两种方式
    - 全部重启：关闭所有Zookeeper服务，修改配置之后启动，不影响之前客户端的会话
    - 逐个重启：这是比较常用的方式。
  - ZooKeeper集群中服务器之间是怎样通信的
    - Leader服务器会和每一个Follower/Observer服务器都建立TCP连接，同时为每个F/O都创建一个叫做LearnerHandler的实体。
    - LearnerHandler主要负责Leader和F/O之间的网络通讯，包括数据同步，请求转发和Proposal提议的投票等。Leader服务器保存了所有F/O的LearnerHandler
  - ZooKeeper是否会自动进行日志清理？如何进行日志清理
    - zk自己不会进行日志清理，需要运维人员进行日志清理
  - 谈谈你对ZooKeeper的理解
    - Zookeeper 作为一个分布式的服务框架，主要用来解决分布式集群中应用系统的一致性问题。ZooKeeper提供的服务包括：分布式消息同步和协调机制、服务器节点动态上下线、统一配置管理、负载均衡、集群管理等
    - ZooKeeper提供基于类似于Linux文件系统的目录节点树方式的数据存储，即分层命名空间。Zookeeper 并不是用来专门存储数据的，它的作用主要是用来维护和监控你存储的数据的状态变化，通过监控这些数据状态的变化，从而可以达到基于数据的集群管理，ZooKeeper节点的数据上限是1MB
    - Zookeeper=文件系统+通知机制
    - 对于ZooKeeper的数据结构，每个子目录项如  NameService  都被称作为  znode，这个  znode是被它所在的路径唯一标识
    - znode 可以有子节点目录，并且每个 znode 可以存储数据，注意 EPHEMERAL 类型的目录节点不能有子节点目录(因为它是临时节点)
    - znode 是有版本的，每个 znode 中存储的数据可以有多个版本，也就是一个访问路径中可以存储多份数据
    - znode 可以是临时节点，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除
    - Zookeeper 的客户端和服务器通信采用长连接方式，每个客户端和服务器通过心跳来保持连接，这个连接状态称为 session，如果 znode 是临时节点，这个 session 失效，znode 也就删除了
    - znode 的目录名可以自动编号
  - ZooKeeper节点类型
    - 短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除
    - 持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除
    - 持久化目录节点（PERSISTENT）
    - 持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL）：客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号
    - 临时目录节点（EPHEMERAL）
    - 临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL）：客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号
  - 请说明ZooKeeper的通知机制
    - ZooKeeper选择了基于通知（notification）的机制，即：客户端向ZooKeeper注册需要接受通知的znode，通过znode设置监控点（watch）来接受通知
    - 监视点是一个单次触发的操作，意即监视点会触发一个通知。为了接收多个通知，客户端必须在每次通知后设置一个新的监视点
    - ![](../jpg/img_31.png)
  - ZooKeeper的监听原理是什么
    - 应用程序中，mian()方法首先会创建zkClient，创建zkClient的同时就会产生两个进程，即Listener进程（监听进程）和connect进程（网络连接/传输进程）
    - 当zkClient调用getChildren()等     方法注册监视器时，connect进程向ZooKeeper注册监听器，注册后的监听器位于ZooKeeper的监听器列表中，监听器列表中记录了zkClient的IP，端口号以及要监控的路径
    - 一旦目标文件发生变化，ZooKeeper就会把这条消息发送给对应的zkClient的Listener()进程
    - Listener进程接收到后，就会执行process()方法
  - 请说明ZooKeeper使用到的各个端口的作用
    - 2888：Follower与Leader交换信息的端口
    - 3888：万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口
  - ZooKeeper的部署方式有哪几种？集群中的角色有哪些？ 集群最少需要几台机器
    - ZooKeeper的部署方式有单机模式和集群模式，集群中的角色有Leader和Follower，集群最少3（2N+1）台，根据选举算法，应保证奇数
  - ZooKeeper集群如果有3台机器，挂掉一台是否还能工作？ 挂掉两台呢
    - 对于ZooKeeper集群，过半存活即可使用
  - ZooKeeper使用的ZAB协议与Paxo算法的异同
    - Paxos算法是分布式选举算法，Zookeeper使用的 ZAB协议（Zookeeper原子广播）
    - 相同：比如都有一个Leader，用来协调N个Follower的运行；Leader要等待超半数的Follower做出正    确反馈之后才进行提案；二者都有一个值来代表Leader的周期
    - 不同：ZAB用来构建高可用的分布式数据主备系统（Zookeeper），Paxos是用来构建分布式一致性状态机系统
  - 请谈谈对ZooKeeper对事务性的支持
    - ZooKeeper对于事务性的支持主要依赖于四个函数，zoo_create_op_init， zoo_delete_op_init， zoo_set_op_init以及zoo_check_op_init
    - 每一个函数都会在客户端初始化一个operation，客户端程序有义务保留这些operations
    - 当准备好一个事务中的所有操作后，可以使用zoo_multi来提交所有的操作，由zookeeper服务来保证这一系列操作的原子性。也就是说只要其中有一个操作失败了，相当于此次提交的任何一个操作都没有对服务端的数据造成影响
    - Zoo_multi的返回值是第一个失败操作的状态信号

- HIVE
  - 基础
    - Hive表关联查询，如何解决数据倾斜的问题
      - 倾斜原因：map输出数据按key Hash的分配到reduce中，由于key分布不均匀、业务数据本身的特、建表时考虑不周、某些SQL语句本身就有数据倾斜等原因造成的reduce 上的数据量差异过大
      - 如何避免
        - 对于key为空产生的数据倾斜，可以对其赋予一个随机值
        - 参数调节 ： hive.map.aggr = true hive.groupby.skewindata=true
          - 有数据倾斜的时候进行负载均衡，当选项设定位true,生成的查询计划会有两个MR Job,第二个MR Job再根据预处理的数据结果按照Group By Key 分布到 Reduce中
        - SQL 语句调节
          - 选用join key分布最均匀的表作为驱动表。做好列裁剪和filter操作，以达到两表做join 的时候，数据量相对变小的效果
          - 大小表Join：使用map join让小的维度表（1000 条以下的记录条数）先进内存。在map端完成reduce.
          - 大表Join大表:把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null 值关联不上，处理后并不影响最终结果
          - count distinct大量相同特殊值:将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union
  - 请谈一下Hive的特点，Hive和RDBMS有什么异同
    - hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析，但是Hive不支持实时查询。
    - ![](../jpg/img_32.png)
  - 请说明hive中 Sort By，Order By，Cluster By，Distrbute By各代表什么意思
    - order by：会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）。只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。
    - sort by：不是全局排序，其在数据进入reducer前完成排序
    - distribute by：按照指定的字段对数据进行划分输出到不同的reduce中。
    - cluster by：除了具有 distribute by 的功能外还兼具 sort by 的功能
  - 简要描述数据库中的 null，说出null在hive底层如何存储，并解释select a.* from t1 a left outer join t2 b on a.id=b.id where b.id is null; 语句的含义
    - null与任何值运算的结果都是null, 可以使用is null、is not null函数指定在其值为null情况下的取值
    - null在hive底层默认是用'\N'来存储的，可以通过alter table test SET SERDEPROPERTIES('serialization.null.format' = 'a');来修改
    - 查询出t1表中与t2表中id不相等的所有信息
  - 写出hive中split、coalesce及collect_list函数的用法（可举例）
    - split将字符串转化为数组，即：split('a,b,c,d' , ',') ==> ["a","b","c","d"]。
    - coalesce(T v1, T v2, …) 返回参数中的第一个非空值；如果所有值都为 NULL，那么返回NULL
    - collect_list列出该字段所有的值，不去重	select collect_list(id) from table
  - Hive有哪些方式保存元数据，各有哪些特点
    - Single User Mode：默认安装hive，hive是使用derby内存数据库保存hive的元数据，这样是不可以并发调用hive的
    - User Mode：通过网络连接到一个数据库中，是最经常使用到的模式。假设使用本机mysql服务器存储元数据。这种存储方式需要在本地运行一个mysql服务器，可并发调用
    - Remote Server Mode：在服务器端启动一个 MetaStoreServer，客户端利用 Thrift 协议通过 MetaStoreServer 访问元数据库
  - Hive内部表和外部表的区别
    - 默认创建的表都是管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下
    - 外部表：Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉
  - Hive的HSQL转换为MapReduce的过程
    - HiveSQL ->AST(抽象语法树) -> QB(查询块) ->OperatorTree（操作树）->优化后的操作树 ->mapreduce任务树->优化后的mapreduce任务树
    - SQL Parser：Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象 语法树AST Tree
    - Semantic Analyzer：遍历AST Tree，抽象出查询的基本组成单元QueryBlock
    - Logical plan：遍历QueryBlock，翻译为执行操作树OperatorTree
    - Logical plan optimizer: 逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量
    - Physical plan：遍历OperatorTree，翻译为MapReduce任务
    - Logical plan optimizer：物理层优化器进行MapReduce任务的变换，生成最终的执行计划
  - Hive底层与数据库交互原理
    - hive的元数据可能要面临不断地更新、修改和读取操作，所以它显然不适合使用Hadoop    文件系统进行存储。目前Hive将元数据存储在RDBMS中，比如存储在MySQL、Derby中。元数据信息包括：存在的表、表的列、权限和更多的其他信息
  - 请把下面语句用Hive实现
  - 写出将 text.txt 文件放入 hive 中 test 表
    - LOAD DATA LOCAL INPATH '/your/path/test.txt' OVERWRITE INTO TABLE test PARTITION (l_date='2016-10-10')
  - Hive如何进行权限控制
    - 目前hive支持简单的权限管理，默认情况下是不开启，这样所有的用户都具有相同的权限，同时也是超级管理员，也就对hive中的所有表都有查看和改动的权利
    - Hive可以是基于元数据的权限管理，也可以基于文件存储级别的权限管理
    - Hive中的角色和平常我们认知的角色是有区别的，Hive中的角色可以理解为一部分有一些相同“属性”的用户或组或角色的集合。这里有个递归的概念，就是一个角色可以是一些角色的集合
  - 对于hive，你写过哪些udf函数，作用是什么
  - Hive 中的压缩格式TextFile、SequenceFile、RCfile 、ORCfile各有什么区别
    - TextFile:默认格式，存储方式为行存储，数据不做压缩，磁盘开销大，数据解析开销大。 可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)
      - 压缩后的文件不支持split，Hive      不会对数据进行切分，从而无法对数据进行并行操作。并且在反序列化过程中，必须逐个字符判断     是不是分隔符和行结束符，因此反序列化开销会比SequenceFile高几十倍。
    - SequenceFile: 存储方式为行存储，其具有使用方便、可分割、可压缩的特点
      - SequenceFile支持三种压缩选择：NONE，RECORD，BLOCK。Record压缩率低，一般建议使    用BLOCK压缩
    - RCFile:数据按行分块，每块按列存储.RCFile 保证同一行的数据位于同一节点，因此元组重构的开销很低.RCFile 能够利用列维度的数据压缩，并且能跳过不必要的列读取
      - 数据追加：RCFile 不支持任意方式的数据写操作，仅提供一种追加接口，这是因为底层的HDFS当前仅仅支持数据追加写文件尾部
    - ORCFile:数据按行分块 每块按照列存储。压缩快 快速列存取
      - 效率比rcfile高,是rcfile的改良版本
    - 数据仓库的特点是一次写入、多次读取，因此，整体来看，RCFILE相比其余两种格式具有较明    显的优势
    - 相比TEXTFILE和SEQUENCEFILE，RCFILE由于列式存储方式，数据加载时性能消耗较大，    但是具有较好的压缩比和查询响应
  - Hive join过程中大表小表的放置顺序
    - 将最大的表放置在JOIN语句的最右边，或者直接使用/*+ streamtable(table_name) */指出。在编写带有 join 操作的代码语句时，应该将条目少的表/子查询放在 Join 操作符的左边
    - 在 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，载入条目较少的表可以有效减少 OOM（out of memory）即内存溢出。所以对于同一个 key 来说，对应的 value 值小的放前，大的放后，这便是“小表放前”原则
  - Hive的两张表关联，使用MapReduce怎么实现
    - 如果其中有一张表为小表，直接使用map端join的方式（map端加载小表）进行聚合
    - 如果两张都是大表，那么采用联合key，联合key的第一个组成部分是join on中的公共字段，第二部分是一个flag，0代表表A，1代表表B，由此让Reduce区分客户信息和订单信息；在Mapper中    同时处理两张表的信息，将join on公共字段相同的数据划分到同一个分区中，进而传递到一个Reduce中，然后在Reduce中实现聚合
  - Hive中使用什么代替in查询
    - 在Hive 0.13版本之前，通过left outer join实现SQL中的in查询，0.13版本之后，Hive已经支持in 查询
  - 所有的Hive任务都会有MapReduce的执行吗
    - 不是，从Hive0.10.0版本开始，对于简单的不需要聚合的类似SELECT <col> from <table> LIMIT n语句，不需要起MapReduce job，直接通过Fetch task获取数据
  - Hive的函数：UDF、UDAF、UDTF的区别
    - UDF（User-Defined-Function） 一进一出
    - UDAF（User-Defined Aggregation Function） 聚集函数，多进一出
    - UDTF（User-Defined Table-Generating Functions） 一进多出 如lateral view explore()
  - 说说对Hive桶表的理解
    - 桶表是对数据进行哈希取值，然后放到不同文件中存储
    - 桶表专门用于抽样查询，是很专业性的
  - Hive自定义UDF函数的流程
    - 写一个类继承（org.apache.hadoop.hive.ql.）UDF类
    - 覆盖方法evaluate()
    - 打JAR包；
    - 通过hive命令将JAR添加到Hive的类路径： hive> add	jar	/home/ubuntu/ToDate.jar
    - 注册函数 hive> create	temporary	function	xxx	as	'XXX';
    - 使用函数；
    - [可选] drop临时函数；
  - Hive可以像关系型数据库那样建立多个库吗
    - 可以
  - Hive实现统计的查询语句是什么
    - count 等
  - Hive优化措施
    - Fetch抓取：Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台
    - 本地模式：对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对     于小数据集，执行时间可以明显被缩短
      - set hive.exec.mode.local.auto=true; //开启本地mr
      - set hive.exec.mode.local.auto.inputbytes.max=50000000;
      - et hive.exec.mode.local.auto.input.files.max=10;
    - 表的优化
      - 小表、大表Join
        - 将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率
        - 可以使用Group让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce
      - 大表Join大表
        - 空KEY过滤
          - 有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer，需要在SQL语句中进行过滤
          - select n.* from (select * from nullidtable where id is not null ) n	left join ori o on n.id = o.id;
        - 空key转换
          - 可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上
          - select n.* from nullidtable n full join ori o on
            case when n.id is null then concat('hive', rand()) else n.id end = o.id;
      - MapJoin
        - 可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理
        - set hive.auto.convert.join = true; 默认为true
        - 大表小表的阀值设置（默认25M一下认为是小表）： set hive.mapjoin.smalltable.filesize=25000000;
      - Group By
        - 当一个key数据过大时就倾斜了。 并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果
        - 当选项设定为 true，生成的查询计划会有两个MR Job
      - Count(Distinct) 去重统计
        - 一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换
        - 虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。
      - 笛卡尔积
        - 尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积
      - 行列过滤
        - 列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *
        - 行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会    先全表关联，之后再过滤
      - 动态分区调整
        - 分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中
        - 开启动态分区参数设置: hive.exec.dynamic.partition=true
        - 设置为非严格模式：hive.exec.dynamic.partition.mode=nonstrict
      - map数目
        - 通常情况下，作业会通过input的目录产生一个或者多个map任务：input的文件总个数，input的文件大小，集群设置的文件块大小
        - 是不是map数越多越好：如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时     间，就会造成很大的资源浪费
        - 是不是保证每个map处理接近128m的文件块，就高枕无忧了：答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定     也比较耗时
      - 小文件进行合并
        - 在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能
        - set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
      - 复杂文件增加Map数
        - 当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使    得每个map处理的数据量减少，从而提高任务的执行效率
        - computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数
        - set mapreduce.input.fileinputformat.split.maxsize=100;
      - Reduce数
        - 调整个数的方
          - 每 个 Reduce 处 理 的 数 据 量 默 认 是 256MB hive.exec.reducers.bytes.per.reducer=256000000
          - 每个任务最大的reduce数，默认为1009 hive.exec.reducers.max=1009
          - 计算reducer数的公式 N=min(参数2，总输入数据量/参数1) 
        - 调整reduce个数方法二
          - 在hadoop的mapred-default.xml文件中修改设置每个job的Reduce个数set mapreduce.job.reduces = 15
        - reduce个数并不是越多越好
          - 过多的启动和初始化reduce也会消耗时间和资源；
          - 另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这    些小文件作为下一个任务的输入，则也会出现小文件过多的问题
  - 并行执行
    - Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段
    - 某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短
    - 通过设置参数hive.exec.parallel值为true，就可以开启并发执行
    - set hive.exec.parallel.thread.number=16;	//同一个sql允许最大并行度，默认为8
  - 严格模式
    - 可以防止用户执行那些可能意向不到的不好的影响的查询
    - hive.mapred.mode值为默认是非严格模式nonstrict 。开启严格模式需要修改hive.mapred.mode值为strict
    - 对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行
    - 对于使用了order by语句的查询，要求必须使用limit语句
    - 限制笛卡尔积的查询
  - JVM重用
    - JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避    免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短
    - JVM重用可以使得JVM     实例在同一个job中重新使用N次
    - 开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task 都结束了才会释放
  - 推测执行
    - 根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果
    - Hadoop的mapred-site.xml文件中进行配置
  - EXPLAIN（执行计划）
    - EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query
- HiveSQL
  - 建表
    - create table student(s_id string,s_name string,s_birth string,s_sex string) row format delimited fields terminated by '\t';
    - create table course(c_id string,c_name string,t_id string) row format delimited fields terminated by '\t';
    - create table teacher(t_id string,t_name string) row format delimited fields terminated by '\t';
    - create table score(s_id string,c_id string,s_score int) row format delimited fields terminated by '\t';
  - 导数据
    - load data local inpath '/export/data/hivedatas/student.csv' into table student;
    - load data local inpath '/export/data/hivedatas/course.csv' into table course;
    - load data local inpath '/export/data/hivedatas/teacher.csv' into table teacher;
    - load data local inpath '/export/data/hivedatas/score.csv' into table score;
  - hive查询语法
    - SELECT [ALL | DISTINCT] select_expr, select_expr, ...
      FROM table_reference
      [WHERE where_condition]
      [GROUP BY col_list [HAVING condition]]
      [CLUSTER BY col_list]| [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]
      [LIMIT number]
  - SQL问题
    - 查询"01"课程比"02"课程成绩高的学生的信息及课程分数
      - join  score a,b on 01 /02 ,取分数字段，做比较
      - select student.*,a.s_score as 01_score,b.s_score as 02_score
        from student
        join score a on student.s_id=a.s_id and a.c_id='01'
        left join score b on student.s_id=b.s_id and b.c_id='02'
        where a.s_score>b.s_score;
    - 查询平均成绩大于等于60分的同学的学生编号和学生姓名和平均成绩
      - student.s_id,student.s_name,tmp.平均成绩 from student
        join (
        select score.s_id,round(avg(score.s_score),1)as 平均成绩from score group by s_id)
        as tmp
        on tmp. 平 均 成 绩 >=60 and student.s_id = tmp.s_id 
    - 查询所有同学的学生编号、学生姓名、选课总数、所有课程的总成绩
      - select student.s_id,student.s_name,(count(score.c_id) )as total_count,sum(score.s_score)as total_score
        from student
        left join score on student.s_id=score.s_id
        group by student.s_id,student.s_name ;
    - 查询学过编号为"01"但是没有学过编号为"02"的课程的同学的信息
      - select student.* from student
        join (select s_id from score where c_id =1 )tmp1 on student.s_id=tmp1.s_id
        left join (select s_id from score where c_id =2 )tmp2 on student.s_id =tmp2.s_id
        where tmp2.s_id is null;
    - 查询没有学全所有课程的同学的信息
      - 先查询出课程的总数量
      - select s_id from score
        group by s_id
        having count(c_id)=3
    - 查询和"01"号的同学学习的课程完全相同的其他同学的信息
      - :hive不支持group_concat方法,可用 concat_ws(’|’, collect_set(str)) 实现
      - select student.*,tmp1.course_id from student
        join (select s_id ,concat_ws('|', collect_set(c_id)) course_id from score group by s_id having s_id not in (1))tmp1
        on student.s_id = tmp1.s_id
        join (select concat_ws('|', collect_set(c_id)) course_id2 from score where s_id=1)tmp2
        on tmp1.course_id = tmp2.course_id2;
    - 按各科成绩进行排序，并显示排名
      - row_number() over()分组排序功能(mysql没有该方法)
      - where score.s_id=student.s_id
        group by score.s_id,s_name order by sumscore desc;
      - 简写形式，笛卡尔积
    - 查询所有课程的成绩第2名到第3名的学生信息及该课程成绩
      - rank between 2 and 3
      - (select * from score where c_id='01' order by s_score desc limit 3)tmp1 order by s_score asc limit 2
    - 
